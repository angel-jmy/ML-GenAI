{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbd1e16-c69f-441f-81a7-985822c3a83f",
   "metadata": {},
   "source": [
    "# Model I/O: Input prompts, model invocation, output parsing.\n",
    "\n",
    "![Model I/O diagram](image.jpg)\n",
    "\n",
    "### 1. Prompt Templates  \n",
    "### 2. Language Models  \n",
    "### 3. Output Parsing\n",
    "\n",
    "**Case:** An application has been developed using a large model that can automatically generate flower copy!\n",
    "\n",
    "Generate a brief instruction for each flower on sale, so whenever your employees or customers want to learn about a particular flower, invoking this template will generate suitable text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3516e3a4-7688-4a50-9855-ef0302357bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['flower_name', 'price'] input_types={} partial_variables={} template='You are a professional florist copywriter.\\n\\n\\nCan you provide an attractive brief description for the {flower_name} priced at ${price}?\\n'\n"
     ]
    }
   ],
   "source": [
    "#1. prompt Templates : prompt Engineering\n",
    "\"\"\"\n",
    "1. Provide the model with clear and concise instructions.\n",
    "2. Allow the model to think slowly\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a professional florist copywriter.\\n\n",
    "\n",
    "Can you provide an attractive brief description for the {flower_name} priced at ${price}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a62b9ea-2a23-4525-ad3c-439489481a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Radiant and romantic, our exquisite rose bouquet priced at $50 is the perfect expression of love and admiration. With its velvety petals and enchanting fragrance, this classic flower symbolizes beauty and passion. Hand-crafted by our skilled florists, each stem is carefully selected and arranged to create a stunning display that will leave a lasting impression. Whether for a special occasion or just to show someone you care, our $50 rose bouquet is a timeless and elegant choice.\"\n"
     ]
    }
   ],
   "source": [
    "#2. Language Models\n",
    "\"\"\"\n",
    "1. LLM: Text Model\n",
    "2. Chat Model\n",
    "3. Embedding Model\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "model = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "\n",
    "input = prompt.format(flower_name='rose', price='50')\n",
    "\n",
    "output = model.invoke(input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61292ed-ed69-4c32-9493-e909ff463c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Indulge in the timeless beauty of our stunning rose, priced at an affordable $50. With delicate petals and vibrant hues, this flower exudes elegance and romance. Perfect for any occasion, it's a luxurious and thoughtful gift that will leave a lasting impression. Handcrafted by our skilled florists, each rose is carefully selected and arranged to create a breathtaking display. Add a touch of sophistication to your loved one's day with this exquisite rose.\"\n",
      "\n",
      "Elevate any occasion with the elegant and timeless beauty of a lily. Our carefully selected lilies, priced at $30, exude sophistication and grace with their delicate petals and subtle fragrance. Perfect for gifting or enhancing any space, these lilies are sure to make a lasting impression. Trust our expert florists to handcraft a stunning arrangement that will leave a lasting impression. Order now and add a touch of luxury to any setting with our exquisite lilies.\n",
      "\n",
      "\"Delicate and affordable, our $20 carnations are a perfect choice for any occasion. With their vibrant colors and long-lasting freshness, these blooms add a touch of elegance to any floral arrangement. Whether it's for a birthday, anniversary, or just to brighten someone's day, these carnations are sure to make a lasting impression. Order now and let these versatile and budget-friendly flowers make your loved ones' day a little more special.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a professional florist copywriter.\\n\n",
    "\n",
    "Can you provide an attractive brief description for the {flower_name} priced at ${price}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# print(prompt)\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "model = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "\n",
    "flowers = ['rose', 'lily', 'carnation']\n",
    "prices = ['50', '30', '20']\n",
    "\n",
    "for flower, price in zip(flowers, prices):\n",
    "    input_prompt = prompt.format(flower_name=flower, price=price)\n",
    "    output = model.invoke(input_prompt)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "815ab03f-390b-48ae-822a-296ec96f178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indulge in the epitome of romance with our stunning rose priced at 50 dollars. Adorned with elegant petals in deep shades of red, this timeless beauty exudes unbridled passion and heartfelt love. Perfect for any occasion, let this luxurious rose ignite the flame of love and leave a lasting impression. Handcrafted by our talented florists, it's a statement piece that will surely make hearts flutter. Grace your loved ones with this exquisite rose and watch them swoon with joy\n",
      "\"Add a touch of elegance to any occasion with our stunning lilies, priced at just $30. These iconic blooms symbolize purity and beauty, making them a timeless addition to any bouquet. With their delicate petals and alluring fragrance, our lilies are guaranteed to make a lasting impression. Perfect for gifting or to enhance your own space, these lilies are a must-have for any flower lover. Order now and let these graceful flowers bring a touch of sophistication to your life.\"\n",
      "Add a touch of elegance and grace to any occasion with our stunning carnations priced at only 20 dollars. These versatile blooms come in a range of vibrant hues, perfect for expressing love, gratitude, or sympathy. Expertly grown and lovingly arranged, our carnations make a beautiful statement at an affordable price. Order now and let their delicate beauty and sweet fragrance delight your loved ones.\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"\"\"You are a professional florist copywriter.\\n\n",
    "\n",
    "Can you provide an attractive brief description for the {} priced at {} dollars?\n",
    "\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI() ### Less flexible model settings\n",
    "\n",
    "flowers = ['rose', 'lily', 'carnation']\n",
    "prices = ['50', '30', '20']\n",
    "\n",
    "for flower, price in zip(flowers, prices):\n",
    "    input_prompt = prompt_text.format(flower, price)\n",
    "    response = client.completions.create(\n",
    "        model='gpt-3.5-turbo-instruct',\n",
    "        prompt=input_prompt,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    print(response.choices[0].text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3363d2f4-7b0e-4f09-b212-b408c8ba63d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InferenceClient' object has no attribute 'post'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m model = HuggingFaceEndpoint(repo_id=\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3-1-8B-Instruct\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28minput\u001b[39m = prompt.format(flower_name=\u001b[33m'\u001b[39m\u001b[33mrose\u001b[39m\u001b[33m'\u001b[39m, price=\u001b[33m'\u001b[39m\u001b[33m50\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1317\u001b[39m, in \u001b[36mBaseLLM.__call__\u001b[39m\u001b[34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[39m\n\u001b[32m   1310\u001b[39m     msg = (\n\u001b[32m   1311\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1312\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1313\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`generate` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1314\u001b[39m     )\n\u001b[32m   1315\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1317\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m     .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m   1326\u001b[39m     .text\n\u001b[32m   1327\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:973\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    959\u001b[39m     run_managers = [\n\u001b[32m    960\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    961\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    971\u001b[39m         )\n\u001b[32m    972\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    981\u001b[39m     run_managers = [\n\u001b[32m    982\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    983\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    990\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    991\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    782\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    783\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    788\u001b[39m     **kwargs: Any,\n\u001b[32m    789\u001b[39m ) -> LLMResult:\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    791\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    796\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    800\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    801\u001b[39m         )\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    803\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1547\u001b[39m, in \u001b[36mLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1544\u001b[39m new_arg_supported = inspect.signature(\u001b[38;5;28mself\u001b[39m._call).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m   1546\u001b[39m     text = (\n\u001b[32m-> \u001b[39m\u001b[32m1547\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1548\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m   1549\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(prompt, stop=stop, **kwargs)\n\u001b[32m   1550\u001b[39m     )\n\u001b[32m   1551\u001b[39m     generations.append([Generation(text=text)])\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\langchain_community\\llms\\huggingface_endpoint.py:267\u001b[39m, in \u001b[36mHuggingFaceEndpoint._call\u001b[39m\u001b[34m(self, prompt, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     invocation_params[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m] = invocation_params[\n\u001b[32m    265\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstop_sequences\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    266\u001b[39m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m(\n\u001b[32m    268\u001b[39m         json={\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m: prompt, \u001b[33m\"\u001b[39m\u001b[33mparameters\u001b[39m\u001b[33m\"\u001b[39m: invocation_params},\n\u001b[32m    269\u001b[39m         stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    270\u001b[39m         task=\u001b[38;5;28mself\u001b[39m.task,\n\u001b[32m    271\u001b[39m     )\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    273\u001b[39m         response_text = json.loads(response.decode())[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'InferenceClient' object has no attribute 'post'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "model = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-1-8B-Instruct\")\n",
    "\n",
    "input = prompt.format(flower_name='rose', price='50')\n",
    "output = model(input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a5b9e60-f334-44a8-8a62-1800f29e4043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langchain prompt\\n1. Readability\\n2. Reusability\\n3. Maintenance\\n4. Variable Handing\\n5. Parameterization\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"langchain prompt\n",
    "1. Readability\n",
    "2. Reusability\n",
    "3. Maintenance\n",
    "4. Variable Handing\n",
    "5. Parameterization\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fe13c83-2c3b-4daf-90b7-d7e8f3eb7369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'flower': 'rose', 'price': '50', 'description': 'This stunning rose is the epitome of elegance and romance. Its vibrant petals in a deep shade of red exude passion and love, making it the perfect gift for your significant other or a special occasion. The intricate layers and delicate fragrance of this rose will leave a lasting impression on anyone who receives it.', 'reason': 'This copy is written to highlight the beauty and symbolism of the rose, while also creating a sense of desire and emotion in the reader. By emphasizing its vibrant color, delicate fragrance, and suitability as a gift, this description aims to entice customers to purchase this rose for their loved ones or themselves.'}, {'flower': 'lily', 'price': '30', 'description': 'Elegant and graceful, the lily is a symbol of purity and devotion. With its delicate petals and sweet fragrance, this stunning flower is perfect for any occasion. At just $30, you can add a touch of sophistication to your floral arrangement without breaking the bank.', 'reason': 'The description highlights the beauty and symbolism of the lily, making it appealing to customers looking for a meaningful and affordable floral option. It also emphasizes the versatility of the flower, making it suitable for any event or sentiment.'}, {'flower': 'carnation', 'price': '20', 'description': 'Add a touch of elegance and sophistication to your floral arrangement with our beautiful carnations. These classic blooms are priced at an affordable $20, making them the perfect choice for any occasion. Their delicate petals and vibrant colors will surely make a statement and brighten up any room.', 'reason': 'This copy highlights the key selling points of the carnation - its elegance, affordability, and ability to enhance any floral arrangement. It also creates a sense of urgency with the mention of their vibrant colors and ability to make a statement, encouraging customers to make a purchase.'}]\n"
     ]
    }
   ],
   "source": [
    "#3. Output Parsing: -> {description, reason:} -> csv\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a professional florist copywriter.\\n\n",
    "Can you provide an attractive brief description for the {flower_name} priced at ${price}?\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "model = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"description\", description=\"Description Copy for flowers\"),\n",
    "    ResponseSchema(name=\"reason\", description=\"Why write this copy this way?\")\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate.from_template(template, partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "flowers = ['rose', 'lily', 'carnation']\n",
    "prices = ['50', '30', '20']\n",
    "# print(prompt)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=[\"flower\", \"price\", \"description\", \"reason\"])\n",
    "\n",
    "for flower, price in zip(flowers, prices):\n",
    "    input_prompt = prompt.format(flower_name=flower, price=price)\n",
    "    output = model(input_prompt)\n",
    "    output_parsed = output_parser.parse(output)\n",
    "\n",
    "    output_parsed[\"flower\"] = flower\n",
    "    output_parsed[\"price\"] = price\n",
    "    df.loc[len(df)] = output_parsed\n",
    "\n",
    "print(df.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf48fea-38a5-4574-9ce0-0ae2454f0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"flowers_copy.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910947a2-e9d1-4e36-86fc-f5c3f9229e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hw:\n",
    "1. How was the format_instructions, the output format, constructed using the output_parser in the example above, and how was it passed to the prompt template?\n",
    "2. With the addition of partial_variables, which is the format_instructions specified by the output parser, why can the model generate structured output?\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

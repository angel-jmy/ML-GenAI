{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangChain AI](https://langchain-ai.github.io/langgraph/agents/memory/\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Mechanism\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "history.add_ai_message(\"whats up?\")\n",
    "\n",
    "history.messages\n",
    "# [HumanMessage(content='hi!'), AIMessage(content='whats up?')]\n",
    "\n",
    "## ConversationChain -> RunnableWithMessageHistory\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# True\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=0.5,\n",
    "    model_name='gpt-3.5-turbo-instruct'  # 4096 Token\n",
    ")\n",
    "\n",
    "conv_chain = ConversationChain(llm=llm)\n",
    "\n",
    "print(conv_chain.prompt.template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using ConversationBufferMemory\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())\n",
    "\n",
    "# round 1\n",
    "conversation(\"My sister's birthday is tomorrow, and I need a birthday bouquet.\")\n",
    "print(conversation)\n",
    "\n",
    "print(conversation.memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round 2\n",
    "conversation(\"She likes pink roses, specifically the color pink.\")\n",
    "print(conversation.memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round 3\n",
    "conversation(\"I'm back again. Do you remember why I came to buy flowers just?\")\n",
    "print(\"template::\", conversation.prompt.template)\n",
    "\n",
    "print(\"memory::\", conversation.memory.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph --> workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import uuid\n",
    "from langchain_openai import OpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(temperature=0.5, model_name='gpt-3.5-turbo-instruct')\n",
    "\n",
    "# Define the Workflow Using StateGraph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add Memory Management\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Set thread ID and config\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Turn Dialogue\n",
    "query_1 = \"My sister's birthday is tomorrow, and I need a birthday bouquet.\"\n",
    "input_messages_1 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a florist. Answer the following questions as best you can.\"},\n",
    "    {\"role\": \"user\", \"content\": query_1},\n",
    "]\n",
    "\n",
    "for event in app.stream({\"messages\": input_messages_1}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"She likes pink roses, specifically the color pink.\"\n",
    "input_messages_2 = [{\"role\": \"user\", \"content\": query_2}]\n",
    "\n",
    "for event in app.stream({\"messages\": input_messages_2}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_3 = \"I'm back again. Do you remember why I came to buy flowers just?\"\n",
    "input_messages_3 = [{\"role\": \"user\", \"content\": query_3}]\n",
    "\n",
    "for event in app.stream({\"messages\": input_messages_3}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conversation Buffer Window: ConversationBufferWindowMemory\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=ConversationBufferWindowMemory(k=1))\n",
    "\n",
    "# round 1\n",
    "result = conversation(\"My sister's birthday is tomorrow, and I need a birthday bouquet.\")\n",
    "print(result)\n",
    "\n",
    "# round 2\n",
    "result = conversation(\"She likes pink roses, specifically the color pink.\")\n",
    "print(result)\n",
    "\n",
    "# round 3\n",
    "result = conversation(\"I'm back again. Do you remember why I came to buy flowers yesterday?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferWindowMemory, ConversationTokenBufferMemory -> trim_messages\n",
    "\n",
    "# use trim_messages - k, n\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "system_message = SystemMessage(\"You are a helpful assistant that remembers recent messages.\")\n",
    "messages = [system_message]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a trimming window size\n",
    "k = 1\n",
    "# messages.append(HumanMessage(content=user_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmed_conversation(user_input):\n",
    "    global messages\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "\n",
    "    trimmed_messages = trim_messages(\n",
    "        messages=messages,\n",
    "        token_counter=len,\n",
    "        max_tokens=k + 1,\n",
    "        strategy=\"last\",\n",
    "        start_on=\"human\",\n",
    "        include_system=True,\n",
    "        allow_partial=False\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(trimmed_messages)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({})\n",
    "\n",
    "    messages.append(AIMessage(content=response))\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1\n",
    "result = trimmed_conversation(\"My sister's birthday is tomorrow, and I need a birthday bouquet.\")\n",
    "print(\"Assistant:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 2\n",
    "result = trimmed_conversation(\"She likes pink roses, specifically the color pink.\")\n",
    "print(\"Assistant:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 3\n",
    "result = trimmed_conversation(\"I'm back again. Do you remember why I came to buy flowers yesterday?\")\n",
    "print(\"Assistant:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationTokenBufferMemory -> trimmed_conversation\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "system_message = SystemMessage(\"You are a helpful assistant that remembers recent messages.\")\n",
    "\n",
    "messages = [system_message]\n",
    "\n",
    "max_tokens = 80\n",
    "\n",
    "def trimmed_conversation(user_input):\n",
    "    global messages\n",
    "\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "\n",
    "    trimmed_messages = trim_messages(\n",
    "        messages=messages,\n",
    "        token_counter=llm,\n",
    "        max_tokens=max_tokens,\n",
    "        strategy=\"last\",\n",
    "        start_on=\"human\",\n",
    "        include_system=True,\n",
    "        allow_partial=False\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(trimmed_messages)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({})\n",
    "\n",
    "    messages.append(AIMessage(content=response))\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Assistant:\", trimmed_conversation(\"My sister's birthday is tomorrow, and I need a birthday bouquet.\"))\n",
    "    print(\"Assistant:\", trimmed_conversation(\"She likes pink roses, specifically the color pink.\"))\n",
    "    print(\"Assistant:\", trimmed_conversation(\"I'm back again. Do you remember why I came to buy flowers yesterday?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "system_message = SystemMessage(\"You are a helpful assistant that remembers recent messages.\")\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        token_counter=llm,\n",
    "        max_tokens=800,\n",
    "        strategy=\"last\",\n",
    "        start_on=\"human\",\n",
    "        include_system=True,\n",
    "        allow_partial=False\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(trimmed_messages)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({})\n",
    "\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=response)]}\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_message = HumanMessage(content=\"My sister's birthday is tomorrow, and I need a birthday bouquet.\")\n",
    "    for event in app.stream({\"messages\": [system_message, input_message]}, config, stream_mode=\"values\"):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    input_message = HumanMessage(content=\"She likes pink roses, specifically the color pink.\")\n",
    "    for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    input_message = HumanMessage(content=\"I'm back again. Do you remember why I came to buy flowers yesterday?\")\n",
    "    for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "        event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationBufferMemory    \n",
    "\n",
    "ConversationBufferWindowMemory    \n",
    "\n",
    "ConversationTokenBufferMemory    \n",
    "\n",
    "LangGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ConversationSummaryMemory\n",
    "\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=ConversationSummaryMemory(llm=llm))\n",
    "\n",
    "# round 1\n",
    "result = conversation(\"My sister's birthday is tomorrow, and I need a birthday bouquet.\")\n",
    "print(result)\n",
    "\n",
    "# round 2\n",
    "result = conversation(\"She likes pink roses, specifically the color pink.\")\n",
    "print(result)\n",
    "\n",
    "# round 3\n",
    "result = conversation(\"I'm back again. Do you remember why I came to buy flowers yesterday?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistence -> Session Summary\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, RemoveMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from typing import Literal\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "def call_model(state: State):\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    return END\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(\"summarize_conversation\", summarize_conversation)\n",
    "\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_update(update):\n",
    "    for k, v in update.items():\n",
    "        for m in v[\"messages\"]:\n",
    "            m.pretty_print()\n",
    "        if \"summary\" in v:\n",
    "            print(v[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "input_message = HumanMessage(content=\"My sister's birthday is tomorrow, and I need a birthday bouquet.\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)\n",
    "\n",
    "input_message = HumanMessage(content=\"She likes pink roses, specifically the color pink.\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)\n",
    "\n",
    "input_message = HumanMessage(content=\"I'm back again. Do you remember why I came to buy flowers yesterday?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = app.get_state(config).values\n",
    "values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are more than 2 messages, you need to summarize the conversation --->   if len(messages) > 2:\n",
    "values = app.get_state(config).values\n",
    "values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=100))\n",
    "\n",
    "# round 1\n",
    "result = conversation(\"My sister's birthday is tomorrow, and I need a birthday bouquet.\")\n",
    "print(result)\n",
    "\n",
    "# round 2\n",
    "result = conversation(\"She likes pink roses, specifically the color pink.\")\n",
    "print(result)\n",
    "\n",
    "# round 3\n",
    "result = conversation(\"I'm back again. Do you remember why I came to buy flowers yesterday?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Memory Comparison](buffer-summary.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Memory Type                        | Advantages                                                                                      | Disadvantages                                                                                         |\n",
    "|-----------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\n",
    "| ConversationSummaryBufferMemory   | Able to recall early interactions<br>Does not miss recent information<br>High level of flexibility | For short conversations, the summarizer may increase token usage.<br>Storing original interactions may also increase token usage. |\n",
    "| ConversationSummaryMemory         | Reduces token usage for long conversations.<br>Allows for longer conversation durations.<br>Straightforward and intuitive | Adding token usage required for the summarizing LLM.<br>Conversation memory depends on the summarizing LLM's aggregation ability |\n",
    "| ConversationBufferWindowMemory    | Retains only the most recent interactions, resulting in fewer token usage.<br>Adjustable window size for flexibility. | Unable to recall early interaction.<br>Too large of a window may result in excessive token usage.     |\n",
    "| ConversationBufferMemory          | Provides the largest amount of information for LLM.<br>Simple and intuitive                      | Uses more tokens, resulting in increased response time and higher costs.<br>Long conversations may exceed token limits. |\n",
    "\n",
    "---\n",
    "\n",
    "**How to test memory behaviors:**\n",
    "1. Try changing the value of `k` in the `ConversationBufferWindowMemory` and increase the number of conversation rounds to observe the memory effect.\n",
    "2. Try changing the `max_token_limit` in the `ConversationSummaryBufferMemory` to observe the memory effect.\n",
    "3. (Updated) Implement these memory techniques using your own business scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "1. Memory Management Mechanisms:\n",
    "   - Conversation Buffer Memory\n",
    "   - Window Memory with key value limitations\n",
    "   - Token Buffer Memory\n",
    "   - Long Graph implementation\n",
    "\n",
    "2. Memory Types:\n",
    "   - Short-term memory\n",
    "   - Long-term memory\n",
    "   - Summary memory\n",
    "\n",
    "3. Implementation Approaches:\n",
    "   - Message handling and truncation\n",
    "   - Conversation summarization techniques\n",
    "   - State management and persistence\n",
    "   - Token limit management\n",
    "   - Memory checkpoints and namespace handling\n",
    "\n",
    "4. Technical Considerations:\n",
    "   - Token usage optimization\n",
    "   - Context window sizing\n",
    "   - History message storage\n",
    "   - Real-time message processing\n",
    "   - Multi-thread support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
